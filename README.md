# ðŸ¥… Task Planning Papers

A collection of task planning papers.

## Papers

[arXiv Oct. 18, 2023] [**Video Language Planning**]
![Robotics](https://img.shields.io/badge/Robotics-blue) ![LLM](https://img.shields.io/badge/LLM-red) ![Multi-Modality](https://img.shields.io/badge/Multi--Modality-orange) <br />
Yilun Du, Mengjiao Yang, Pete Florence, Fei Xia, Ayzaan Wahid, Brian Ichter, Pierre Sermanet, Tianhe Yu, Pieter Abbeel, Joshua B. Tenenbaum, Leslie Kaelbling, Andy Zeng, Jonathan Tompson <br />
> We are interested in enabling visual planning for complex long-horizon tasks in the space of generated videos and language, leveraging recent advances in large generative models pretrained on Internet-scale data. To this end, we present video language planning (VLP), an algorithm that consists of a tree search procedure, where we train (i) vision-language models to serve as both policies and value functions, and (ii) text-to-video models as dynamics models. VLP takes as input a long-horizon task instruction and current image observation, and outputs a long video plan that provides detailed multimodal (video and language) specifications that describe how to complete the final task. VLP scales with increasing computation budget where more computation time results in improved video plans, and is able to synthesize long-horizon video plans across different robotics domains: from multi-object rearrangement, to multi-camera bi-arm dexterous manipulation. Generated video plans can be translated into real robot actions via goal-conditioned policies, conditioned on each intermediate frame of the generated video. Experiments show that VLP substantially improves long-horizon task success rates compared to prior methods on both simulated and real robots (across 3 hardware platforms).

[arXiv Oct. 16, 2023] [**Interactive Task Planning with Language Models**]
![Robotics](https://img.shields.io/badge/Robotics-blue) ![LLM](https://img.shields.io/badge/LLM-red) ![Multi-Modality](https://img.shields.io/badge/Multi--Modality-orange) <br />
Boyi Li, Philipp Wu, Pieter Abbeel, Jitendra Malik <br />
> An interactive robot framework accomplishes long-horizon task planning and can easily generalize to new goals or distinct tasks, even during execution. However, most traditional methods require predefined module design, which makes it hard to generalize to different goals. Recent large language model based approaches can allow for more open-ended planning but often require heavy prompt engineering or domain-specific pretrained models. To tackle this, we propose a simple framework that achieves interactive task planning with language models. Our system incorporates both high-level planning and low-level function execution via language. We verify the robustness of our system in generating novel high-level instructions for unseen objectives and its ease of adaptation to different tasks by merely substituting the task guidelines, without the need for additional complex prompt engineering. Furthermore, when the user sends a new request, our system is able to replan accordingly with precision based on the new request, task guidelines and previously executed steps. Please check more details on our [this https URL](https://wuphilipp.github.io/itp_site) and [this https URL](https://youtu.be/TrKLuyv26_g).

[arXiv Aug. 29, 2023] [**TaskLAMA: Probing the Complex Task Understanding of Language Models**](https://arxiv.org/abs/2308.15299)
![Resource](https://img.shields.io/badge/Resource-green) ![LLM](https://img.shields.io/badge/LLM-red)<br />
Quan Yuan, Mehran Kazemi, Xin Xu, Isaac Noble, Vaiva Imbrasaite, Deepak Ramachandran <br />
[[Dataset]](https://storage.googleapis.com/gresearch/tasklama/tasklama.zip) <br />
> Structured Complex Task Decomposition (SCTD) is the problem of breaking down a complex real-world task (such as planning a wedding) into a directed acyclic graph over individual steps that contribute to achieving the task, with edges specifying temporal dependencies between them. SCTD is an important component of assistive planning tools, and a challenge for commonsense reasoning systems. We probe how accurately SCTD can be done with the knowledge extracted from Large Language Models (LLMs). We introduce a high-quality human-annotated dataset for this problem and novel metrics to fairly assess performance of LLMs against several baselines. Our experiments reveal that LLMs are able to decompose complex tasks into individual steps effectively, with a relative improvement of 15% to 280% over the best baseline. We also propose a number of approaches to further improve their performance, with a relative improvement of 7% to 37% over the base model. However, we find that LLMs still struggle to predict pairwise temporal dependencies, which reveals a gap in their understanding of complex tasks.
